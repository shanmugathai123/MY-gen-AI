{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxyODVGBT7AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLussxMlKwhh",
        "outputId": "9373eb6b-958b-48ee-8b2e-6d876e8d4fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter one word: machine\n",
            "✅ Predicted next word: learning\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Step 2: Training text\n",
        "text = \"machine learning is powerful. deep learning is a branch of machine learning. artificial intelligence includes machine learning.\"\n",
        "\n",
        "# Step 3: Tokenize words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "index_word = {v: k for k, v in word_index.items()}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Step 4: Prepare sequences (2-word to 1-word)\n",
        "words = text.lower().split()\n",
        "sequences = []\n",
        "\n",
        "for i in range(1, len(words)):\n",
        "    seq = words[i-1:i+1]\n",
        "    encoded = tokenizer.texts_to_sequences([' '.join(seq)])[0]\n",
        "    if len(encoded) == 2:\n",
        "        sequences.append(encoded)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "x, y = sequences[:, 0], sequences[:, 1]\n",
        "x = x.reshape((x.shape[0], 1))\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# Step 5: Build and train model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=1))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(x, y, epochs=300, verbose=0)\n",
        "\n",
        "# Step 6: Predict next word\n",
        "input_word = input(\"Enter one word: \").lower()\n",
        "\n",
        "if input_word not in word_index:\n",
        "    print(\"❌ Word not found in training text!\")\n",
        "else:\n",
        "    encoded = tokenizer.texts_to_sequences([input_word])[0]\n",
        "    encoded = np.array(encoded).reshape(1, 1)\n",
        "\n",
        "    pred = model.predict(encoded, verbose=0)\n",
        "    next_index = np.argmax(pred)\n",
        "    predicted_word = index_word.get(next_index, \"<unknown>\")\n",
        "    print(\"✅ Predicted next word:\", predicted_word)\n"
      ]
    }
  ]
}