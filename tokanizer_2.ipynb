{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KiX8S5g4Pvsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wfE-uLEOeqt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=[\"i love deep learning\",\"RNNs are powerful for sequence data\",\"Tokenization is the first step\"]"
      ],
      "metadata": {
        "id": "oxfNsS1OPxsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()"
      ],
      "metadata": {
        "id": "VBOs8WOJQV5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(text)"
      ],
      "metadata": {
        "id": "7NiivIEkQeXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence=tokenizer.texts_to_sequences(text)"
      ],
      "metadata": {
        "id": "KWYkeHOHQtTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequence=pad_sequences(sequence,padding=\"post\")"
      ],
      "metadata": {
        "id": "tdGaS4CERGSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"word_index\\n\",tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAdTNSomRPNS",
        "outputId": "362cc372-c206-415c-d737-2ad939938fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_index\n",
            " {'i': 1, 'love': 2, 'deep': 3, 'learning': 4, 'rnns': 5, 'are': 6, 'powerful': 7, 'for': 8, 'sequence': 9, 'data': 10, 'tokenization': 11, 'is': 12, 'the': 13, 'first': 14, 'step': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"sequence:\\n\",sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa4ManETSNnm",
        "outputId": "2eb06536-01cc-4172-e2fc-bd407613b330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence:\n",
            " [[1, 2, 3, 4], [5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"paddedsequence\\n\",padded_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWCSB_H3jW1h",
        "outputId": "c56b2182-fa89-4550-81bb-032f5d5f0e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paddedsequence\n",
            " [[ 1  2  3  4  0  0]\n",
            " [ 5  6  7  8  9 10]\n",
            " [11 12 13 14 15  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"this is sample sentance fro tokenization demo\"\n",
        "word_tokens=text.split()\n",
        "print(\"word tokens:\",word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTMmVdOs-FMH",
        "outputId": "cd2cbab9-a1e5-4424-bc95-e9e805c1c5bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word tokens: ['this', 'is', 'sample', 'sentance', 'fro', 'tokenization', 'demo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_token=list(text)\n",
        "print(\"character token\",char_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtQVu0IU-hGd",
        "outputId": "80507f97-5f6d-490b-b767-0e19f9d4d40a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "character token ['t', 'h', 'i', 's', ' ', 'i', 's', ' ', 's', 'a', 'm', 'p', 'l', 'e', ' ', 's', 'e', 'n', 't', 'a', 'n', 'c', 'e', ' ', 'f', 'r', 'o', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'd', 'e', 'm', 'o']\n"
          ]
        }
      ]
    }
  ]
}